<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Segment as You Wish--Free-Form Language-Based Segmentation for Medical Images</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Segment as You Wish: Free-Form Language-Based Segmentation for Medical Images</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://longchaoda.github.io/LongchaoHere/">Longchao Da*</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Rui Wang*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Xiaojian Xu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Parminder Bhatia</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Taha Kass-Hout</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Hua Wei</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Cao Xiao</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Arizona State University,</span>
            <span class="author-block"><sup>2</sup>GE-Healthcare</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.12831"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.12831"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <!-- <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA" -->
                <a href="https://youtu.be/CvAVgxK9eZI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/LongchaoDa/SegmentAsYouWish"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="#code-section"
                    class="external-link button is-normal is-rounded is-dark"
                    onclick="scrollToCodeSection(event)">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/fig1.jpg"
            class="interpolation-image"
            alt="Interpolate start reference image."
            width="1400" height="300"/>
      <h2 class="subtitle has-text-centered">
         The Demo for two types of free form language prompt segmentations.
      </h2>
    </div>
  </div>


  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/arcti44.png"
            class="interpolation-image"
            alt="Interpolate start reference image."
            width="1400" height="300"/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Architecture</span> The architecture of the FLanS for Free Form language-prompted segmentation on Medical Images
      </h2>
    </div>
  </div>



</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Medical imaging is crucial for diagnosing a patient's health condition, and accurate segmentation of these images is essential for isolating regions of interest to ensure precise diagnosis and treatment planning. Existing methods primarily rely on bounding boxes or point-based prompts, while few have explored text-related prompts, despite clinicians often describing their observations and instructions in natural language.
          </p>
          <p>
            To address this gap, we first propose a RAG-based free-form text prompt generator, that leverages the domain corpus to generate diverse and realistic descriptions. Then, we introduce FLanS, a novel medical image segmentation model that handles various free-form text prompts, including professional anatomy-informed queries, anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven queries. 
          </p>
          <p>
            Additionally, our model also incorporates a symmetry-aware canonicalization module to ensure consistent, accurate segmentations across varying scan orientations and reduce confusion between the anatomical position of an organ and its appearance in the scan.
          </p>
          <p>
            FLanS is trained on a large-scale dataset of over 100k medical images from 7 public datasets. Comprehensive experiments demonstrate the model's superior language understanding and segmentation precision, along with a deep comprehension of the relationship between them, outperforming SOTA baselines on both in-domain and out-of-domain datasets.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!--/ GIF. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Anatomy-Informed Segmentation - Expert</h2>
        
    <h5 class="title is-3">EMR snippet 1 (pseudonymized)</h5>
        <!-- <img src="./static/videos/case1.mp4"
            class="interpolation-image"
            alt="Interpolate start reference image."
            width="1400" height="300"/>
           -->
           <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/case1.mp4"
                    type="video/mp4">
          </video>
      </div>
    </div>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h5 class="title is-3">EMR snippet 2 (pseudonymized)</h5>
        <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
          <source src="./static/videos/case2.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Normal Query 1</h2>
          <!-- <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p> -->
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/NQ1.mp4"
                    type="video/mp4">
          </video>
          <!-- static/images/MRI-Brain-T1Gd_rotate.gif -->
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Normal Query 2</h2>
        <div class="columns is-centered">
          <div class="column content">
            <!-- <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p> -->
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/NQ2.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Normal Query 3</h2>
          <!-- <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p> -->
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/NQ3.mp4"
                    type="video/mp4">
          </video>
          <!-- static/images/MRI-Brain-T1Gd_rotate.gif -->
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Normal Query 4</h2>
        <div class="columns is-centered">
          <div class="column content">
            <!-- <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p> -->
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/NQ4.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <!-- Animation. -->
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div id="code-section" class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            Dataset Links we have used:
          </p>
          <!-- # replace with dataset links -->
              <p>
                <a href="https://arxiv.org/abs/2104.09125">1. Pancreas-CT</a> This dataset consists of 82 contrast-enhanced abdominal CT volumes, only provides the pancreas label annotated by an experienced radiologist, and all CT scans have no pancreatic tumor.
              </p>
              <p>
                <a href="https://drive.google.com/drive/folders/0B0vscETPGI1-Q1h1WFdEM2FHSUE?resourcekey=0-XIVV_7YUjB9TPTQ3NfM17A">2. LiTS</a> contains 131 and 70 contrast-enhanced 3-D abdominal CT scans for training and testing, respectively. The data set was acquired by different scanners and protocols at six different clinical sites, with a largely varying in-plane resolution from 0.55 to 1.0 mm and slice spacing from 0.45 to 6.0 mm.
              </p>
              <p>
                <a href="https://kits-challenge.org/kits23/#download-block">3. KiTS</a> It includes 210 training cases and 90 testing cases with annotations provided by the University of Minnesota Medical Center. Each CT scan has one or more kidney tumors.
              </p>
              <p>
                <a href="https://github.com/JunMa11/AbdomenCT-1K">4. AbdomenCT-1K</a> It consists of 1112 CT scans from five datasets with liver, kidney, spleen, and pancreas annotations.
              </p>
              <p>
                <a href="https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=61080890">5. CT-ORG</a> It is composed of 140 CT images containing 6 organ classes, which are from 8 different medical centers. Most of the images exhibit liver lesions, both benign and malignant.
              </p>
              <p>
                <a href="https://chaos.grand-challenge.org/Data/">6. CHAOS</a> It provides 20 patients for multi-organ segmentation. All CT scans have no liver tumor.
              </p>
              <p>
                <a href="https://www.nature.com/articles/s41467-022-30695-9/">7. MSD CT</a> It includes liver, lung, pancreas, colon, hepatic vessel, and spleen tasks for a total of 947 CT scans with 4 organs and 5 tumors.
              </p>
              <p>
                <a href="https://www.synapse.org/Synapse:syn3193805/wiki/217789/">8. BTCV</a> It consists of 50 abdominal CT scans from metastatic liver cancer patients or post-operative ventral hernia patients. They are collected from the Vanderbilt University Medical Center.
              </p>
              <p>
                <a href="https://amos22.grand-challenge.org/">9. AMOS22</a> It is the abbreviation of the multi-modality abdominal multi-organ segmentation challenge of 2022. The AMOS dataset contains 500 CT with voxel-level annotations of 15 abdominal organs.
              </p>
              <p>
                <a href="https://github.com/HiLab-git/WORD/">10. WORD</a> It collects 150 CT scans from 150 patients before the radiation therapy in a single center, all of them are scanned by a SIEMENS CT scanner without appearance enhancement. Each CT volume consists of 159 to 330 slices of 512 × 512 pixels.
              </p>
              <p>
                <a href="https://www.ircad.fr/research/data-sets/liver-segmentation-3d-ircadb-01/">11. 3D-IRCADb</a> It contains 20 venous phase enhanced CT scans. Each CT scan has various annotations, and only annotated organs are tested to validate the model’s generalizability.
              </p>
              <p>
                <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10546353/">12. TotalSegmentator</a> It collects 1024 CT scans randomly sampled from PACS over the timespan of the last 10 years. The dataset contains CT images with different sequences (native, arterial, portal venous, late phase, dual-energy), with and without contrast agent, with different bulb voltages, with different slice thicknesses and resolution and with different kernels.
              </p>
              <p>
                <a href="https://zenodo.org/records/7860267">12. FLARE22Train</a> It is a dataset used as the labeled training set in MICCAI FLARE 2022 Challenge https://flare22.grand-challenge.org/.
              </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<!-- here to add yours -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{da2024segment,
      title={Segment as You Wish--Free-Form Language-Based Segmentation for Medical Images},
      author={Da, Longchao and Wang, Rui and Xu, Xiaojian and Bhatia, Parminder and Kass-Hout, Taha and Wei, Hua and Xiao, Cao},
      journal={arXiv preprint arXiv:2410.12831},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2410.12831">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/LongchaoDa" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
